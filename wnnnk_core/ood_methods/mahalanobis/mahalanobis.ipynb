{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6f31d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "======================================================================\n",
      "Mahalanobis Distance OOD Detection\n",
      "Device: cuda\n",
      "======================================================================\n",
      "\n",
      "[Step 1/4] Loading ID validation data...\n",
      "  Loading: C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\imagenet1k_activations\\resnet50_imagenet1k_val_avgpool.pt\n",
      "    Shape: (50000, 2048)\n",
      "\n",
      "[Step 2/4] Fitting Mahalanobis parameters...\n",
      "Fitting Mahalanobis parameters...\n",
      "  Computing class means...\n",
      "    Processed 100/1000 classes\n",
      "    Processed 200/1000 classes\n",
      "    Processed 300/1000 classes\n",
      "    Processed 400/1000 classes\n",
      "    Processed 500/1000 classes\n",
      "    Processed 600/1000 classes\n",
      "    Processed 700/1000 classes\n",
      "    Processed 800/1000 classes\n",
      "    Processed 900/1000 classes\n",
      "    Processed 1000/1000 classes\n",
      "  Completed all 1000 classes\n",
      "  Computing tied covariance matrix...\n",
      "  Fitting covariance estimator...\n",
      "  Precision matrix shape: (2048, 2048)\n",
      "  Condition number: 1.20e+04\n",
      "\n",
      "[Step 3/4] Computing ID scores...\n",
      "  Computing Mahalanobis scores for 50000 samples (batch_size=500)...\n",
      "    Progress: 5000/50000 (10.0%)\n",
      "    Progress: 10000/50000 (20.0%)\n",
      "    Progress: 15000/50000 (30.0%)\n",
      "    Progress: 20000/50000 (40.0%)\n",
      "    Progress: 25000/50000 (50.0%)\n",
      "    Progress: 30000/50000 (60.0%)\n",
      "    Progress: 35000/50000 (70.0%)\n",
      "    Progress: 40000/50000 (80.0%)\n",
      "    Progress: 45000/50000 (90.0%)\n",
      "    Progress: 50000/50000 (100.0%)\n",
      "  ID scores: min=-5979.7686, max=-246.5245, mean=-1010.0749\n",
      "\n",
      "[Step 4/4] Evaluating OOD datasets...\n",
      "======================================================================\n",
      "\n",
      "OOD Dataset 1/4: iNaturalist\n",
      "  Loading: C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\selected_inaturalist21_activations\\resnet50_inaturalist21_10k_avgpool.pt\n",
      "    Shape: (10000, 2048)\n",
      "  Computing Mahalanobis scores for 10000 samples (batch_size=500)...\n",
      "    Progress: 1000/10000 (10.0%)\n",
      "    Progress: 2000/10000 (20.0%)\n",
      "    Progress: 3000/10000 (30.0%)\n",
      "    Progress: 4000/10000 (40.0%)\n",
      "    Progress: 5000/10000 (50.0%)\n",
      "    Progress: 6000/10000 (60.0%)\n",
      "    Progress: 7000/10000 (70.0%)\n",
      "    Progress: 8000/10000 (80.0%)\n",
      "    Progress: 9000/10000 (90.0%)\n",
      "    Progress: 10000/10000 (100.0%)\n",
      "  OOD scores: min=-4097.5811, max=-505.3188, mean=-1223.7863\n",
      "  ✓ Results: FPR95=88.87%, AUROC=69.61%\n",
      "\n",
      "OOD Dataset 2/4: SUN\n",
      "  Loading: C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\selected_sun_activations\\resnet50_sun_10k_avgpool.pt\n",
      "    Shape: (10000, 2048)\n",
      "  Computing Mahalanobis scores for 10000 samples (batch_size=500)...\n",
      "    Progress: 1000/10000 (10.0%)\n",
      "    Progress: 2000/10000 (20.0%)\n",
      "    Progress: 3000/10000 (30.0%)\n",
      "    Progress: 4000/10000 (40.0%)\n",
      "    Progress: 5000/10000 (50.0%)\n",
      "    Progress: 6000/10000 (60.0%)\n",
      "    Progress: 7000/10000 (70.0%)\n",
      "    Progress: 8000/10000 (80.0%)\n",
      "    Progress: 9000/10000 (90.0%)\n",
      "    Progress: 10000/10000 (100.0%)\n",
      "  OOD scores: min=-3702.2319, max=-407.5260, mean=-1109.6309\n",
      "  ✓ Results: FPR95=94.01%, AUROC=60.32%\n",
      "\n",
      "OOD Dataset 3/4: Places\n",
      "  Loading: C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\selected_places_activations\\resnet50_places_10k_avgpool.pt\n",
      "    Shape: (10000, 2048)\n",
      "  Computing Mahalanobis scores for 10000 samples (batch_size=500)...\n",
      "    Progress: 1000/10000 (10.0%)\n",
      "    Progress: 2000/10000 (20.0%)\n",
      "    Progress: 3000/10000 (30.0%)\n",
      "    Progress: 4000/10000 (40.0%)\n",
      "    Progress: 5000/10000 (50.0%)\n",
      "    Progress: 6000/10000 (60.0%)\n",
      "    Progress: 7000/10000 (70.0%)\n",
      "    Progress: 8000/10000 (80.0%)\n",
      "    Progress: 9000/10000 (90.0%)\n",
      "    Progress: 10000/10000 (100.0%)\n",
      "  OOD scores: min=-3834.6729, max=-413.7085, mean=-1105.0352\n",
      "  ✓ Results: FPR95=94.62%, AUROC=59.98%\n",
      "\n",
      "OOD Dataset 4/4: Textures\n",
      "  Loading: C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\textures_activations\\resnet50_textures_avgpool.pt\n",
      "    Shape: (5640, 2048)\n",
      "  Computing Mahalanobis scores for 5640 samples (batch_size=500)...\n",
      "    Progress: 500/5640 (8.3%)\n",
      "    Progress: 1000/5640 (16.7%)\n",
      "    Progress: 1500/5640 (25.0%)\n",
      "    Progress: 2000/5640 (33.3%)\n",
      "    Progress: 2500/5640 (41.7%)\n",
      "    Progress: 3000/5640 (50.0%)\n",
      "    Progress: 3500/5640 (58.3%)\n",
      "    Progress: 4000/5640 (66.7%)\n",
      "    Progress: 4500/5640 (75.0%)\n",
      "    Progress: 5000/5640 (83.3%)\n",
      "    Progress: 5500/5640 (91.7%)\n",
      "    Progress: 5640/5640 (100.0%)\n",
      "  OOD scores: min=-9237.9893, max=-488.1591, mean=-2067.4265\n",
      "  ✓ Results: FPR95=32.62%, AUROC=93.12%\n",
      "\n",
      "======================================================================\n",
      "Mahalanobis Distance OOD Detection Results\n",
      "======================================================================\n",
      "OOD Dataset               FPR95 ↓      AUROC ↑      Samples\n",
      "----------------------------------------------------------------------\n",
      "iNaturalist                88.87%       69.61%        10000\n",
      "SUN                        94.01%       60.32%        10000\n",
      "Places                     94.62%       59.98%        10000\n",
      "Textures                   32.62%       93.12%         5640\n",
      "----------------------------------------------------------------------\n",
      "Average                    77.53%       70.76%            -\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Summary:\n",
      "  iNaturalist: FPR95=88.87%, AUROC=69.61%\n",
      "  SUN: FPR95=94.01%, AUROC=60.32%\n",
      "  Places: FPR95=94.62%, AUROC=59.98%\n",
      "  Textures: FPR95=32.62%, AUROC=93.12%\n",
      "\n",
      "Average: FPR95=77.53%, AUROC=70.76%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def compute_mahalanobis_scores_batch(features, means, precision, num_classes, batch_size=500):\n",
    "    \"\"\"\n",
    "    Compute Mahalanobis scores in batches following the paper's approach.\n",
    "    Much faster than the loop-based version.\n",
    "    \n",
    "    Args:\n",
    "        features: numpy array (N, feature_dim)\n",
    "        means: numpy array (num_classes, feature_dim)\n",
    "        precision: numpy array (feature_dim, feature_dim)\n",
    "        num_classes: int\n",
    "        batch_size: int, number of samples to process at once\n",
    "    \n",
    "    Returns:\n",
    "        scores: numpy array (N,)\n",
    "    \"\"\"\n",
    "    # Convert to torch tensors on GPU\n",
    "    features_torch = torch.from_numpy(features).float().cuda()\n",
    "    means_torch = torch.from_numpy(means).float().cuda()\n",
    "    precision_torch = torch.from_numpy(precision).float().cuda()\n",
    "    \n",
    "    num_samples = features.shape[0]\n",
    "    all_scores = []\n",
    "    \n",
    "    print(f\"  Computing Mahalanobis scores for {num_samples} samples (batch_size={batch_size})...\")\n",
    "    \n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        \n",
    "        batch_features = features_torch[start_idx:end_idx]  # (batch_size, feature_dim)\n",
    "        \n",
    "        gaussian_scores = []\n",
    "        \n",
    "        # Compute distance to each class mean\n",
    "        for cls in range(num_classes):\n",
    "            batch_sample_mean = means_torch[cls]  # (feature_dim,)\n",
    "            zero_f = batch_features - batch_sample_mean  # (batch_size, feature_dim)\n",
    "            \n",
    "            # Mahalanobis distance: -0.5 * (zero_f @ precision @ zero_f.T).diag()\n",
    "            term_gau = -0.5 * torch.mm(torch.mm(zero_f, precision_torch), zero_f.t()).diag()\n",
    "            gaussian_scores.append(term_gau.view(-1, 1))\n",
    "        \n",
    "        # Stack all class scores and take max\n",
    "        gaussian_scores = torch.cat(gaussian_scores, dim=1)  # (batch_size, num_classes)\n",
    "        batch_scores, _ = torch.max(gaussian_scores, dim=1)\n",
    "        \n",
    "        all_scores.append(batch_scores.cpu().numpy())\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % max(1, num_batches // 10) == 0 or (batch_idx + 1) == num_batches:\n",
    "            progress = (batch_idx + 1) / num_batches * 100\n",
    "            print(f\"    Progress: {end_idx}/{num_samples} ({progress:.1f}%)\")\n",
    "    \n",
    "    scores = np.concatenate(all_scores)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def fit_mahalanobis_params(features, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Compute sample means and tied covariance following the paper.\n",
    "    \n",
    "    Args:\n",
    "        features: numpy array (N, feature_dim)\n",
    "        labels: numpy array (N,)\n",
    "        num_classes: int\n",
    "    \n",
    "    Returns:\n",
    "        sample_class_mean: numpy array (num_classes, feature_dim)\n",
    "        precision: numpy array (feature_dim, feature_dim)\n",
    "    \"\"\"\n",
    "    print(\"Fitting Mahalanobis parameters...\")\n",
    "    \n",
    "    feature_dim = features.shape[1]\n",
    "    sample_class_mean = np.zeros((num_classes, feature_dim))\n",
    "    \n",
    "    # Compute per-class means\n",
    "    print(\"  Computing class means...\")\n",
    "    for cls in range(num_classes):\n",
    "        class_mask = (labels == cls)\n",
    "        class_features = features[class_mask]\n",
    "        if len(class_features) > 0:\n",
    "            sample_class_mean[cls] = class_features.mean(axis=0)\n",
    "        \n",
    "        if (cls + 1) % 100 == 0:\n",
    "            print(f\"    Processed {cls+1}/{num_classes} classes\")\n",
    "    \n",
    "    print(f\"  Completed all {num_classes} classes\")\n",
    "    \n",
    "    # Compute tied covariance (same as paper's group_lasso approach)\n",
    "    print(\"  Computing tied covariance matrix...\")\n",
    "    \n",
    "    # Center features by class means\n",
    "    X = []\n",
    "    for cls in range(num_classes):\n",
    "        class_mask = (labels == cls)\n",
    "        class_features = features[class_mask]\n",
    "        if len(class_features) > 0:\n",
    "            X_cls = class_features - sample_class_mean[cls]\n",
    "            X.append(X_cls)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    \n",
    "    # Fit empirical covariance\n",
    "    print(\"  Fitting covariance estimator...\")\n",
    "    group_lasso = EmpiricalCovariance(assume_centered=False)\n",
    "    group_lasso.fit(X)\n",
    "    precision = group_lasso.precision_\n",
    "    \n",
    "    print(f\"  Precision matrix shape: {precision.shape}\")\n",
    "    print(f\"  Condition number: {np.linalg.cond(precision):.2e}\")\n",
    "    \n",
    "    return sample_class_mean, precision\n",
    "\n",
    "\n",
    "def compute_auroc(id_scores, ood_scores):\n",
    "    \"\"\"Compute AUROC.\"\"\"\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    labels = np.concatenate([np.ones(len(id_scores)), np.zeros(len(ood_scores))])\n",
    "    return roc_auc_score(labels, scores) * 100\n",
    "\n",
    "\n",
    "def compute_fpr95(id_scores, ood_scores):\n",
    "    \"\"\"Compute FPR@95.\"\"\"\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    labels = np.concatenate([np.ones(len(id_scores)), np.zeros(len(ood_scores))])\n",
    "    fpr, tpr, _ = roc_curve(labels, scores, pos_label=1)\n",
    "    idx = np.argmax(tpr >= 0.95)\n",
    "    return fpr[idx] * 100\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    \"\"\"Load embeddings from your format.\"\"\"\n",
    "    print(f\"  Loading: {path}\")\n",
    "    data = torch.load(path, map_location='cpu')\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        activations = data['activations']\n",
    "        labels = data.get('labels', None)\n",
    "        \n",
    "        if isinstance(activations, torch.Tensor):\n",
    "            activations = activations.numpy()\n",
    "        if labels is not None and isinstance(labels, torch.Tensor):\n",
    "            labels = labels.numpy()\n",
    "    else:\n",
    "        activations = data.numpy() if isinstance(data, torch.Tensor) else data\n",
    "        labels = None\n",
    "    \n",
    "    print(f\"    Shape: {activations.shape}\")\n",
    "    return activations, labels\n",
    "\n",
    "\n",
    "def evaluate_mahalanobis_ood(id_val_path, ood_dict, num_classes=1000, batch_size=500):\n",
    "    \"\"\"\n",
    "    Evaluate Mahalanobis OOD detection following the paper's implementation.\n",
    "    \n",
    "    Args:\n",
    "        id_val_path: Path to ID validation embeddings (with labels)\n",
    "        ood_dict: Dictionary {ood_name: ood_path}\n",
    "        num_classes: Number of classes\n",
    "        batch_size: Batch size for computing scores\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Mahalanobis Distance OOD Detection\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load ID validation data\n",
    "    print(f\"\\n[Step 1/4] Loading ID validation data...\")\n",
    "    val_activations, val_labels = load_embeddings(id_val_path)\n",
    "    \n",
    "    if val_labels is None:\n",
    "        raise ValueError(\"ID validation data must have labels!\")\n",
    "    \n",
    "    val_features = val_activations.astype(np.float32)\n",
    "    val_labels = val_labels.astype(np.int64)\n",
    "    \n",
    "    # Fit Mahalanobis parameters\n",
    "    print(f\"\\n[Step 2/4] Fitting Mahalanobis parameters...\")\n",
    "    sample_mean, precision = fit_mahalanobis_params(val_features, val_labels, num_classes)\n",
    "    \n",
    "    # Compute ID scores\n",
    "    print(f\"\\n[Step 3/4] Computing ID scores...\")\n",
    "    id_scores = compute_mahalanobis_scores_batch(\n",
    "        val_features, sample_mean, precision, num_classes, batch_size\n",
    "    )\n",
    "    print(f\"  ID scores: min={id_scores.min():.4f}, max={id_scores.max():.4f}, mean={id_scores.mean():.4f}\")\n",
    "    \n",
    "    # Evaluate OOD datasets\n",
    "    print(f\"\\n[Step 4/4] Evaluating OOD datasets...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for ood_idx, (ood_name, ood_path) in enumerate(ood_dict.items(), 1):\n",
    "        print(f\"\\nOOD Dataset {ood_idx}/{len(ood_dict)}: {ood_name}\")\n",
    "        \n",
    "        ood_activations, _ = load_embeddings(ood_path)\n",
    "        ood_features = ood_activations.astype(np.float32)\n",
    "        \n",
    "        ood_scores = compute_mahalanobis_scores_batch(\n",
    "            ood_features, sample_mean, precision, num_classes, batch_size\n",
    "        )\n",
    "        print(f\"  OOD scores: min={ood_scores.min():.4f}, max={ood_scores.max():.4f}, mean={ood_scores.mean():.4f}\")\n",
    "        \n",
    "        auroc = compute_auroc(id_scores, ood_scores)\n",
    "        fpr95 = compute_fpr95(id_scores, ood_scores)\n",
    "        \n",
    "        results[ood_name] = {\n",
    "            'fpr95': fpr95,\n",
    "            'auroc': auroc,\n",
    "            'num_samples': len(ood_features)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Results: FPR95={fpr95:.2f}%, AUROC={auroc:.2f}%\")\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_fpr95 = np.mean([r['fpr95'] for r in results.values()])\n",
    "    avg_auroc = np.mean([r['auroc'] for r in results.values()])\n",
    "    results['average'] = {'fpr95': avg_fpr95, 'auroc': avg_auroc}\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Mahalanobis Distance OOD Detection Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'OOD Dataset':<20} {'FPR95 ↓':>12} {'AUROC ↑':>12} {'Samples':>12}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    for ood_name, metrics in results.items():\n",
    "        if ood_name == 'average':\n",
    "            continue\n",
    "        print(f\"{ood_name:<20} {metrics['fpr95']:>11.2f}% {metrics['auroc']:>11.2f}% {metrics['num_samples']:>12}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"{'Average':<20} {avg_fpr95:>11.2f}% {avg_auroc:>11.2f}% {'-':>12}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "id_val_path = r\"C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\imagenet1k_activations\\resnet50_imagenet1k_val_avgpool.pt\"\n",
    "\n",
    "ood_dict = {\n",
    "    \"iNaturalist\": r\"C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\selected_inaturalist21_activations\\resnet50_inaturalist21_10k_avgpool.pt\",\n",
    "    \"SUN\": r\"C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\selected_sun_activations\\resnet50_sun_10k_avgpool.pt\",\n",
    "    \"Places\": r\"C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\selected_places_activations\\resnet50_places_10k_avgpool.pt\",\n",
    "    \"Textures\": r\"C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\activations\\textures_activations\\resnet50_textures_avgpool.pt\",\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "# batch_size=500 processes 500 samples at once (adjust based on GPU memory)\n",
    "results = evaluate_mahalanobis_ood(\n",
    "    id_val_path=id_val_path,\n",
    "    ood_dict=ood_dict,\n",
    "    num_classes=1000,\n",
    "    batch_size=500  # Increase if you have more GPU memory\n",
    ")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for ood_name in ood_dict.keys():\n",
    "    print(f\"  {ood_name}: FPR95={results[ood_name]['fpr95']:.2f}%, AUROC={results[ood_name]['auroc']:.2f}%\")\n",
    "print(f\"\\nAverage: FPR95={results['average']['fpr95']:.2f}%, AUROC={results['average']['auroc']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
