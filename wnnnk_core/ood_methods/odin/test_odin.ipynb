{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61796a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from E:\\datasets\\ImageNet\\ILSVRC2012_img_val...\n",
      "Using structured (ImageFolder) dataset loading\n",
      "Dataset size: 50000\n",
      "Loading dataset from C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\datasets\\iNaturalist\\images...\n",
      "Using flat dataset loading\n",
      "Found 20000 images\n",
      "Dataset size: 20000\n",
      "\n",
      "Computing ID scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing ODIN scores: 100%|██████████| 782/782 [11:28<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing OOD scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing ODIN scores: 100%|██████████| 313/313 [03:59<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ODIN OOD Detection Results:\n",
      "ID samples: 50000\n",
      "OOD samples: 20000\n",
      "AUROC: 90.96%\n",
      "AUPR: 95.97%\n",
      "FPR95: 42.49%\n",
      "\n",
      "ID confidence - Mean: 0.0010, Std: 0.0000\n",
      "OOD confidence - Mean: 0.0010, Std: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "class FlatImageDataset(Dataset):\n",
    "    \"\"\"Dataset for loading all images from a flat directory structure\"\"\"\n",
    "    def __init__(self, root_dir, transform=None, extensions=('.jpg', '.jpeg', '.png', '.JPEG', '.JPG', '.PNG')):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image files recursively\n",
    "        self.image_paths = []\n",
    "        for ext in extensions:\n",
    "            self.image_paths.extend(list(self.root_dir.rglob(f'*{ext}')))\n",
    "        \n",
    "        self.image_paths.sort()  # For reproducibility\n",
    "        print(f\"Found {len(self.image_paths)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return dummy label (0) since we only care about logits for OOD detection\n",
    "        return image, 0\n",
    "\n",
    "\n",
    "class ODINPostprocessor:\n",
    "    def __init__(self, temperature=1000, noise=0.0014, input_std=[0.229, 0.224, 0.225]):\n",
    "        \"\"\"\n",
    "        ODIN (Out-of-DIstribution detector for Neural networks) postprocessor\n",
    "        \n",
    "        Args:\n",
    "            temperature: Temperature scaling parameter (default: 1000)\n",
    "            noise: Magnitude of input perturbation (default: 0.0014)\n",
    "            input_std: Standard deviation used for normalization (default: ImageNet std)\n",
    "        \"\"\"\n",
    "        self.temperature = temperature\n",
    "        self.noise = noise\n",
    "        self.input_std = input_std\n",
    "\n",
    "    def postprocess_logits(self, model, data, device='cuda'):\n",
    "        \"\"\"\n",
    "        Apply ODIN postprocessing to get confidence scores\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model\n",
    "            data: Input images [batch_size, 3, H, W]\n",
    "            device: Device to run on\n",
    "            \n",
    "        Returns:\n",
    "            pred: Predicted classes\n",
    "            conf: ODIN confidence scores\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        data = data.to(device)\n",
    "        data.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get predicted labels\n",
    "        labels = output.detach().argmax(axis=1)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        output = output / self.temperature\n",
    "        \n",
    "        # Calculate loss and gradients\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalize gradient to binary {-1, 1}\n",
    "        gradient = torch.ge(data.grad.detach(), 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Scale gradient by input std\n",
    "        gradient[:, 0] = gradient[:, 0] / self.input_std[0]\n",
    "        gradient[:, 1] = gradient[:, 1] / self.input_std[1]\n",
    "        gradient[:, 2] = gradient[:, 2] / self.input_std[2]\n",
    "        \n",
    "        # Add perturbation to input\n",
    "        perturbed_data = torch.add(data.detach(), gradient, alpha=-self.noise)\n",
    "        \n",
    "        # Forward pass with perturbed input\n",
    "        output = model(perturbed_data)\n",
    "        output = output / self.temperature\n",
    "        \n",
    "        # Calculate confidence scores\n",
    "        nnOutput = output.detach()\n",
    "        nnOutput = nnOutput - nnOutput.max(dim=1, keepdims=True).values\n",
    "        nnOutput = nnOutput.exp() / nnOutput.exp().sum(dim=1, keepdims=True)\n",
    "        \n",
    "        conf, pred = nnOutput.max(dim=1)\n",
    "        \n",
    "        return pred, conf\n",
    "\n",
    "    def compute_scores_from_images(self, model, dataloader, device='cuda'):\n",
    "        \"\"\"\n",
    "        Compute ODIN scores for a dataset\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model\n",
    "            dataloader: DataLoader with images\n",
    "            device: Device to run on\n",
    "            \n",
    "        Returns:\n",
    "            pred_list: Predictions\n",
    "            conf_list: ODIN confidence scores\n",
    "        \"\"\"\n",
    "        pred_list, conf_list = [], []\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Computing ODIN scores\"):\n",
    "            images = batch[0] if isinstance(batch, (list, tuple)) else batch['data']\n",
    "            images = images.to(device)\n",
    "            \n",
    "            pred, conf = self.postprocess_logits(model, images, device)\n",
    "            \n",
    "            pred_list.append(pred.cpu())\n",
    "            conf_list.append(conf.cpu())\n",
    "        \n",
    "        pred_list = torch.cat(pred_list).numpy().astype(int)\n",
    "        conf_list = torch.cat(conf_list).numpy()\n",
    "        \n",
    "        return pred_list, conf_list\n",
    "\n",
    "\n",
    "def load_dataset(dataset_path, transform, flat_structure=False, batch_size=64, num_workers=0):\n",
    "    \"\"\"\n",
    "    Load dataset with either ImageFolder or FlatImageDataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset directory\n",
    "        transform: Torchvision transforms to apply\n",
    "        flat_structure: If True, use FlatImageDataset. If False, use ImageFolder\n",
    "        batch_size: Batch size for DataLoader\n",
    "        num_workers: Number of workers for DataLoader\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader for the dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from {dataset_path}...\")\n",
    "    print(f\"Using {'flat' if flat_structure else 'structured (ImageFolder)'} dataset loading\")\n",
    "    \n",
    "    if flat_structure:\n",
    "        dataset = FlatImageDataset(dataset_path, transform=transform)\n",
    "    else:\n",
    "        dataset = ImageFolder(dataset_path, transform=transform)\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def compute_ood_metrics(id_conf, ood_conf):\n",
    "    \"\"\"\n",
    "    Compute OOD detection metrics\n",
    "    \n",
    "    Args:\n",
    "        id_conf: Confidence scores for in-distribution data\n",
    "        ood_conf: Confidence scores for out-of-distribution data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with FPR95, AUROC, AUPR metrics\n",
    "    \"\"\"\n",
    "    # Create labels (1 for ID, 0 for OOD)\n",
    "    labels = np.concatenate([np.ones_like(id_conf), np.zeros_like(ood_conf)])\n",
    "    scores = np.concatenate([id_conf, ood_conf])\n",
    "    \n",
    "    # Compute AUROC\n",
    "    auroc = roc_auc_score(labels, scores) * 100\n",
    "    \n",
    "    # Compute AUPR\n",
    "    aupr = average_precision_score(labels, scores) * 100\n",
    "    \n",
    "    # Compute FPR95 (FPR at 95% TPR)\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    fpr95 = fpr[np.argmax(tpr >= 0.95)] * 100\n",
    "    \n",
    "    return {\n",
    "        'AUROC': auroc,\n",
    "        'AUPR': aupr,\n",
    "        'FPR95': fpr95\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    import torchvision\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define transforms (same as used for logit extraction)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load ID dataset (structured - ImageFolder)\n",
    "    id_loader = load_dataset(\n",
    "        dataset_path=r'E:\\datasets\\ImageNet\\ILSVRC2012_img_val',\n",
    "        transform=transform,\n",
    "        flat_structure=False,  # Structured dataset\n",
    "        batch_size=64,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Load OOD dataset (flat structure)\n",
    "    ood_loader = load_dataset(\n",
    "        dataset_path=r'C:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\experiments\\exp6_deep_inversion_for_ood\\data\\datasets\\iNaturalist\\images',\n",
    "        transform=transform,\n",
    "        flat_structure=True,  # Flat dataset\n",
    "        batch_size=64,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Initialize ODIN postprocessor\n",
    "    odin = ODINPostprocessor(temperature=1000, noise=0.0014)\n",
    "    \n",
    "    # Compute ODIN scores\n",
    "    print(\"\\nComputing ID scores...\")\n",
    "    id_preds, id_conf = odin.compute_scores_from_images(model, id_loader, device)\n",
    "    \n",
    "    print(\"\\nComputing OOD scores...\")\n",
    "    ood_preds, ood_conf = odin.compute_scores_from_images(model, ood_loader, device)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_ood_metrics(id_conf, ood_conf)\n",
    "    \n",
    "    print(f\"\\nODIN OOD Detection Results:\")\n",
    "    print(f\"ID samples: {len(id_conf)}\")\n",
    "    print(f\"OOD samples: {len(ood_conf)}\")\n",
    "    print(f\"AUROC: {metrics['AUROC']:.2f}%\")\n",
    "    print(f\"AUPR: {metrics['AUPR']:.2f}%\")\n",
    "    print(f\"FPR95: {metrics['FPR95']:.2f}%\")\n",
    "    print(f\"\\nID confidence - Mean: {id_conf.mean():.4f}, Std: {id_conf.std():.4f}\")\n",
    "    print(f\"OOD confidence - Mean: {ood_conf.mean():.4f}, Std: {ood_conf.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90604ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gabri\\Local Desktop\\Research\\wnnnk\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ID dataset...\n",
      "ID dataset: 50000 samples\n",
      "\n",
      "Loading OOD datasets...\n",
      "\n",
      "iNaturalist:\n",
      "  Found 20000 images\n",
      "\n",
      "SUN:\n",
      "  Found 20000 images\n",
      "\n",
      "Places:\n",
      "  Found 20000 images\n",
      "\n",
      "Textures:\n",
      "======================================================================\n",
      "ODIN OOD Detection\n",
      "Temperature: 1000\n",
      "Magnitude (epsilon): 0.0014\n",
      "======================================================================\n",
      "\n",
      "[Step 1/2] Computing ODIN scores for ID data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_19872\\2671611323.py:51: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out = None)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha = 1, Tensor out = None) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1707.)\n",
      "  tempInputs = torch.add(inputs.data, -noiseMagnitude1, gradient)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 6400/50000 (12.8%)\n",
      "  Progress: 12800/50000 (25.6%)\n",
      "  Progress: 19200/50000 (38.4%)\n",
      "  Progress: 25600/50000 (51.2%)\n",
      "  Progress: 32000/50000 (64.0%)\n",
      "  Progress: 38400/50000 (76.8%)\n",
      "  Progress: 44800/50000 (89.6%)\n",
      "\n",
      "ID ODIN scores: min=0.0010, max=0.0011, mean=0.0010\n",
      "\n",
      "[Step 2/2] Evaluating on OOD datasets...\n",
      "======================================================================\n",
      "\n",
      "OOD Dataset 1/4: iNaturalist\n",
      "  Progress: 6400/20000 (32.0%)\n",
      "  Progress: 12800/20000 (64.0%)\n",
      "  Progress: 19200/20000 (96.0%)\n",
      "  OOD ODIN scores: min=0.0010, max=0.0010, mean=0.0010\n",
      "  ✓ Results: FPR95=41.93%, AUROC=92.24%\n",
      "\n",
      "OOD Dataset 2/4: SUN\n",
      "  Progress: 6400/20000 (32.0%)\n",
      "  Progress: 12800/20000 (64.0%)\n",
      "  Progress: 19200/20000 (96.0%)\n",
      "  OOD ODIN scores: min=0.0010, max=0.0010, mean=0.0010\n",
      "  ✓ Results: FPR95=57.17%, AUROC=86.76%\n",
      "\n",
      "OOD Dataset 3/4: Places\n",
      "  Progress: 6400/20000 (32.0%)\n",
      "  Progress: 12800/20000 (64.0%)\n",
      "  Progress: 19200/20000 (96.0%)\n",
      "  OOD ODIN scores: min=0.0010, max=0.0010, mean=0.0010\n",
      "  ✓ Results: FPR95=64.72%, AUROC=84.11%\n",
      "\n",
      "OOD Dataset 4/4: Textures\n",
      "  OOD ODIN scores: min=0.0010, max=0.0010, mean=0.0010\n",
      "  ✓ Results: FPR95=47.38%, AUROC=87.80%\n",
      "\n",
      "======================================================================\n",
      "ODIN OOD Detection Results\n",
      "======================================================================\n",
      "OOD Dataset               FPR95 ↓      AUROC ↑      Samples\n",
      "----------------------------------------------------------------------\n",
      "iNaturalist                41.93%       92.24%        20000\n",
      "SUN                        57.17%       86.76%        20000\n",
      "Places                     64.72%       84.11%        20000\n",
      "Textures                   47.38%       87.80%         5640\n",
      "----------------------------------------------------------------------\n",
      "Average                    52.80%       87.73%            -\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Final Summary:\n",
      "  iNaturalist: FPR95=41.93%, AUROC=92.24%\n",
      "  SUN: FPR95=57.17%, AUROC=86.76%\n",
      "  Places: FPR95=64.72%, AUROC=84.11%\n",
      "  Textures: FPR95=47.38%, AUROC=87.80%\n",
      "\n",
      "Average: FPR95=52.80%, AUROC=87.73%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def get_odin_score(inputs, model, forward_func, method_args):\n",
    "    \"\"\"\n",
    "    Compute ODIN scores following the paper implementation.\n",
    "    \n",
    "    Args:\n",
    "        inputs: torch.Tensor (batch_size, C, H, W)\n",
    "        model: torch.nn.Module\n",
    "        forward_func: function that takes (inputs, model) and returns logits\n",
    "        method_args: dict with 'temperature' and 'magnitude' keys\n",
    "    \n",
    "    Returns:\n",
    "        scores: numpy array (batch_size,)\n",
    "    \"\"\"\n",
    "    temper = method_args['temperature']\n",
    "    noiseMagnitude1 = method_args['magnitude']\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    inputs = torch.autograd.Variable(inputs, requires_grad=True)\n",
    "    outputs = forward_func(inputs, model)\n",
    "    \n",
    "    maxIndexTemp = np.argmax(outputs.data.cpu().numpy(), axis=1)\n",
    "    \n",
    "    # Using temperature scaling\n",
    "    outputs = outputs / temper\n",
    "    \n",
    "    labels = torch.autograd.Variable(torch.LongTensor(maxIndexTemp).cuda())\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Normalizing the gradient to binary in {0, 1}\n",
    "    gradient = torch.ge(inputs.grad.data, 0)\n",
    "    gradient = (gradient.float() - 0.5) * 2\n",
    "    \n",
    "    # Adding small perturbations to images\n",
    "    tempInputs = torch.add(inputs.data, -noiseMagnitude1, gradient)\n",
    "    \n",
    "    # Forward pass on perturbed inputs\n",
    "    with torch.no_grad():\n",
    "        outputs = forward_func(tempInputs, model)\n",
    "    \n",
    "    outputs = outputs / temper\n",
    "    \n",
    "    # Calculating the confidence after adding perturbations\n",
    "    nnOutputs = outputs.data.cpu()\n",
    "    nnOutputs = nnOutputs.numpy()\n",
    "    nnOutputs = nnOutputs - np.max(nnOutputs, axis=1, keepdims=True)\n",
    "    nnOutputs = np.exp(nnOutputs) / np.sum(np.exp(nnOutputs), axis=1, keepdims=True)\n",
    "    \n",
    "    scores = np.max(nnOutputs, axis=1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def forward_func(inputs, model):\n",
    "    \"\"\"Simple forward function.\"\"\"\n",
    "    return model(inputs)\n",
    "\n",
    "\n",
    "def compute_auroc(id_scores, ood_scores):\n",
    "    \"\"\"Compute AUROC.\"\"\"\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    labels = np.concatenate([np.ones(len(id_scores)), np.zeros(len(ood_scores))])\n",
    "    return roc_auc_score(labels, scores) * 100\n",
    "\n",
    "\n",
    "def compute_fpr95(id_scores, ood_scores):\n",
    "    \"\"\"Compute FPR@95.\"\"\"\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    labels = np.concatenate([np.ones(len(id_scores)), np.zeros(len(ood_scores))])\n",
    "    fpr, tpr, _ = roc_curve(labels, scores, pos_label=1)\n",
    "    idx = np.argmax(tpr >= 0.95)\n",
    "    return fpr[idx] * 100\n",
    "\n",
    "\n",
    "class FlatImageDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from flat directory structure.\"\"\"\n",
    "    def __init__(self, root_dir, transform=None, extensions=('.jpg', '.jpeg', '.png', '.JPEG', '.JPG', '.PNG')):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_paths = []\n",
    "        for ext in extensions:\n",
    "            self.image_paths.extend(list(self.root_dir.rglob(f'*{ext}')))\n",
    "        \n",
    "        self.image_paths.sort()\n",
    "        print(f\"  Found {len(self.image_paths)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, 0  # Dummy label\n",
    "\n",
    "\n",
    "def evaluate_odin_ood(model, id_loader, ood_loaders_dict, temperature=1000, magnitude=0.0014):\n",
    "    \"\"\"\n",
    "    Evaluate ODIN OOD detection.\n",
    "    \n",
    "    Args:\n",
    "        model: torch.nn.Module\n",
    "        id_loader: DataLoader for ID data\n",
    "        ood_loaders_dict: dict {ood_name: ood_loader}\n",
    "        temperature: float - temperature scaling\n",
    "        magnitude: float - perturbation magnitude (epsilon)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ODIN OOD Detection\")\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    print(f\"Magnitude (epsilon): {magnitude}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    method_args = {\n",
    "        'temperature': temperature,\n",
    "        'magnitude': magnitude\n",
    "    }\n",
    "    \n",
    "    # Compute ODIN scores for ID data\n",
    "    print(f\"\\n[Step 1/2] Computing ODIN scores for ID data...\")\n",
    "    id_scores = []\n",
    "    \n",
    "    for batch_idx, (inputs, _) in enumerate(id_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        batch_scores = get_odin_score(inputs, model, forward_func, method_args)\n",
    "        id_scores.append(batch_scores)\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            processed = (batch_idx + 1) * id_loader.batch_size\n",
    "            total = len(id_loader.dataset)\n",
    "            print(f\"  Progress: {processed}/{total} ({100*processed/total:.1f}%)\")\n",
    "    \n",
    "    id_scores = np.concatenate(id_scores)\n",
    "    print(f\"\\nID ODIN scores: min={id_scores.min():.4f}, max={id_scores.max():.4f}, mean={id_scores.mean():.4f}\")\n",
    "    \n",
    "    # Evaluate on each OOD dataset\n",
    "    print(f\"\\n[Step 2/2] Evaluating on OOD datasets...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for ood_idx, (ood_name, ood_loader) in enumerate(ood_loaders_dict.items(), 1):\n",
    "        print(f\"\\nOOD Dataset {ood_idx}/{len(ood_loaders_dict)}: {ood_name}\")\n",
    "        \n",
    "        ood_scores = []\n",
    "        for batch_idx, (inputs, _) in enumerate(ood_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            batch_scores = get_odin_score(inputs, model, forward_func, method_args)\n",
    "            ood_scores.append(batch_scores)\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                processed = (batch_idx + 1) * ood_loader.batch_size\n",
    "                total = len(ood_loader.dataset)\n",
    "                print(f\"  Progress: {processed}/{total} ({100*processed/total:.1f}%)\")\n",
    "        \n",
    "        ood_scores = np.concatenate(ood_scores)\n",
    "        print(f\"  OOD ODIN scores: min={ood_scores.min():.4f}, max={ood_scores.max():.4f}, mean={ood_scores.mean():.4f}\")\n",
    "        \n",
    "        # Compute metrics\n",
    "        auroc = compute_auroc(id_scores, ood_scores)\n",
    "        fpr95 = compute_fpr95(id_scores, ood_scores)\n",
    "        \n",
    "        results[ood_name] = {\n",
    "            'fpr95': fpr95,\n",
    "            'auroc': auroc,\n",
    "            'num_samples': len(ood_scores)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Results: FPR95={fpr95:.2f}%, AUROC={auroc:.2f}%\")\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_fpr95 = np.mean([r['fpr95'] for r in results.values()])\n",
    "    avg_auroc = np.mean([r['auroc'] for r in results.values()])\n",
    "    results['average'] = {'fpr95': avg_fpr95, 'auroc': avg_auroc}\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ODIN OOD Detection Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'OOD Dataset':<20} {'FPR95 ↓':>12} {'AUROC ↑':>12} {'Samples':>12}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    for ood_name, metrics in results.items():\n",
    "        if ood_name == 'average':\n",
    "            continue\n",
    "        print(f\"{ood_name:<20} {metrics['fpr95']:>11.2f}% {metrics['auroc']:>11.2f}% {metrics['num_samples']:>12}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"{'Average':<20} {avg_fpr95:>11.2f}% {avg_auroc:>11.2f}% {'-':>12}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Load model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define transforms (same as used for logit extraction)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "# ID dataset (ImageNet validation)\n",
    "print(\"\\nLoading ID dataset...\")\n",
    "id_dataset_path = r\"E:\\datasets\\ImageNet\\ILSVRC2012_img_val\"\n",
    "id_dataset = ImageFolder(id_dataset_path, transform=transform)\n",
    "id_loader = DataLoader(id_dataset, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
    "print(f\"ID dataset: {len(id_dataset)} samples\")\n",
    "\n",
    "# OOD datasets\n",
    "print(\"\\nLoading OOD datasets...\")\n",
    "ood_datasets = {\n",
    "    \"iNaturalist\": r\"E:\\datasets\\KNN-OOD\\iNaturalist\\images\",\n",
    "    \"SUN\": r\"E:\\datasets\\KNN-OOD\\SUN\\images\",\n",
    "    \"Places\": r\"E:\\datasets\\KNN-OOD\\Places\\images\",\n",
    "    \"Textures\": r\"E:\\datasets\\KNN-OOD\\Textures\\images\",\n",
    "}\n",
    "\n",
    "ood_loaders_dict = {}\n",
    "for ood_name, ood_path in ood_datasets.items():\n",
    "    print(f\"\\n{ood_name}:\")\n",
    "    # Use FlatImageDataset if structure is flat, ImageFolder if structured\n",
    "    try:\n",
    "        # Try ImageFolder first (if has class subdirectories)\n",
    "        dataset = ImageFolder(ood_path, transform=transform)\n",
    "    except:\n",
    "        # Fall back to FlatImageDataset (flat structure)\n",
    "        dataset = FlatImageDataset(ood_path, transform=transform)\n",
    "    \n",
    "    ood_loaders_dict[ood_name] = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=128, \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "# Paper uses: temperature=1000, magnitude=0.0014 for ImageNet\n",
    "results = evaluate_odin_ood(\n",
    "    model=model,\n",
    "    id_loader=id_loader,\n",
    "    ood_loaders_dict=ood_loaders_dict,\n",
    "    temperature=1000,\n",
    "    magnitude=0.0014\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(\"\\nFinal Summary:\")\n",
    "for ood_name in ood_datasets.keys():\n",
    "    print(f\"  {ood_name}: FPR95={results[ood_name]['fpr95']:.2f}%, AUROC={results[ood_name]['auroc']:.2f}%\")\n",
    "print(f\"\\nAverage: FPR95={results['average']['fpr95']:.2f}%, AUROC={results['average']['auroc']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
